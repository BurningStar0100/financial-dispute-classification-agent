{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "\n",
        "# Download necessary NLTK data\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Load your generated dataset\n",
        "# IMPORTANT: After generating the data, save it as 'training_disputes.csv' and upload it to your Colab session.\n",
        "try:\n",
        "    df = pd.read_csv('training_disputes.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'training_disputes.csv' not found. Please upload the generated training data.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Dataset loaded successfully. Shape:\", df.shape)\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Text preprocessing function\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and prepares text data for vectorization.\"\"\"\n",
        "    text = re.sub(r'\\W', ' ', str(text))  # Remove all non-word characters\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)  # Replace multiple spaces with a single space\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = text.split()  # Tokenize\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]  # Stemming and stop word removal\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply preprocessing to the description column\n",
        "df['processed_description'] = df['description'].apply(preprocess_text)\n",
        "print(\"\\nPreprocessing complete. Example:\")\n",
        "print(f\"Original: {df['description'].iloc[0]}\")\n",
        "print(f\"Processed: {df['processed_description'].iloc[0]}\")\n",
        "\n",
        "\n",
        "# --- 2. Feature Engineering and Label Encoding ---\n",
        "\n",
        "# The feature engineered columns are already in the dataset.\n",
        "# We need to define which columns are our features.\n",
        "engineered_features = [\n",
        "    'is_verified_duplicate',\n",
        "    'is_verified_failed',\n",
        "    'contains_fraud_keyword',\n",
        "    'contains_refund_keyword',\n",
        "    'contains_duplicate_keyword'\n",
        "]\n",
        "\n",
        "# Encode the target variable ('true_category') into numbers\n",
        "label_encoder = LabelEncoder()\n",
        "df['category_encoded'] = label_encoder.fit_transform(df['true_category'])\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X_text = df['processed_description']\n",
        "X_engineered = df[engineered_features]\n",
        "y = df['category_encoded']\n",
        "\n",
        "\n",
        "# --- 3. Model Training ---\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_text, X_test_text, X_train_eng, X_test_eng, y_train, y_test = train_test_split(\n",
        "    X_text, X_engineered, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize and fit the TF-IDF Vectorizer on the training text data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=500) # Limit features to the top 500 words\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
        "\n",
        "# Combine TF-IDF features with our engineered features\n",
        "# We convert the sparse TF-IDF matrix to a dense array and concatenate\n",
        "X_train_combined = pd.concat([pd.DataFrame(X_train_tfidf.toarray()), X_train_eng.reset_index(drop=True)], axis=1)\n",
        "X_test_combined = pd.concat([pd.DataFrame(X_test_tfidf.toarray()), X_test_eng.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# The column names get messed up during concat, so we convert them to strings\n",
        "X_train_combined.columns = X_train_combined.columns.astype(str)\n",
        "X_test_combined.columns = X_test_combined.columns.astype(str)\n",
        "\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model.fit(X_train_combined, y_train)\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "\n",
        "# --- 4. Model Evaluation ---\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_combined)\n",
        "y_pred_proba = model.predict_proba(X_test_combined)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n",
        "\n",
        "# --- 5. Saving Model and Artifacts ---\n",
        "\n",
        "# Save the trained model, the vectorizer, and the label encoder for later use\n",
        "joblib.dump(model, 'dispute_classifier_model.pkl')\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "joblib.dump(engineered_features, 'engineered_features.pkl') # Save the list of feature names\n",
        "\n",
        "print(\"\\nModel, TF-IDF vectorizer, label encoder, and feature list have been saved successfully.\")\n",
        "print(\"Files created: dispute_classifier_model.pkl, tfidf_vectorizer.pkl, label_encoder.pkl, engineered_features.pkl\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully. Shape: (967, 8)\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "  dispute_id                                        description  \\\n",
            "0      D1049  Was charged again for the same order, looks li...   \n",
            "1      D1754  emi card was charged twice this month (not sur...   \n",
            "2      D1789  Chargebacckr equested as I on't rrecognize thi...   \n",
            "3      D1733  EMI card was charged twice this month (not sur...   \n",
            "4      D1403  failed transaction but money taken out of my s...   \n",
            "\n",
            "   is_verified_duplicate  is_verified_failed  contains_fraud_keyword  \\\n",
            "0                      1                   0                       0   \n",
            "1                      0                   0                       0   \n",
            "2                      0                   0                       0   \n",
            "3                      0                   0                       0   \n",
            "4                      0                   1                       0   \n",
            "\n",
            "   contains_refund_keyword  contains_duplicate_keyword       true_category  \n",
            "0                        0                           1    DUPLICATE_CHARGE  \n",
            "1                        0                           1              OTHERS  \n",
            "2                        0                           0              OTHERS  \n",
            "3                        0                           1              OTHERS  \n",
            "4                        0                           0  FAILED_TRANSACTION  \n",
            "\n",
            "Preprocessing complete. Example:\n",
            "Original: Was charged again for the same order, looks like a duplicate payment. I have the SMS and bank statement.\n",
            "Processed: charg order look like duplic payment sm bank statement\n",
            "\n",
            "Model training complete.\n",
            "\n",
            "Model Accuracy: 0.9845\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "  DUPLICATE_CHARGE       1.00      1.00      1.00        48\n",
            "FAILED_TRANSACTION       1.00      0.95      0.97        57\n",
            "             FRAUD       1.00      1.00      1.00        32\n",
            "            OTHERS       0.90      1.00      0.95        19\n",
            "    REFUND_PENDING       0.97      1.00      0.99        38\n",
            "\n",
            "          accuracy                           0.98       194\n",
            "         macro avg       0.98      0.99      0.98       194\n",
            "      weighted avg       0.99      0.98      0.98       194\n",
            "\n",
            "\n",
            "Model, TF-IDF vectorizer, label encoder, and feature list have been saved successfully.\n",
            "Files created: dispute_classifier_model.pkl, tfidf_vectorizer.pkl, label_encoder.pkl, engineered_features.pkl\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS4zaoQk7BZL",
        "outputId": "aa90d46a-3069-4fe9-dbbc-b477d4d678df"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}